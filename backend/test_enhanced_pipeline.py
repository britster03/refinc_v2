#!/usr/bin/env python3
"""
Enhanced Pipeline Test Script

Tests all components of our enhanced analysis pipeline:
1. Vector Store with Critical Mass Strategy
2. Iterative Analysis Manager
3. Assessment Coordinator with User Preferences
4. API Endpoints Integration
"""

import asyncio
import sys
import os
from datetime import datetime

# Add the backend directory to the path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

async def test_vector_store_critical_mass():
    """Test vector store critical mass strategy"""
    print("üß™ Testing Vector Store Critical Mass Strategy...")
    
    try:
        from ai_agents.vector_store import VectorStoreManager
        
        # Initialize vector store
        vector_store = VectorStoreManager()
        
        # Test critical mass checking
        readiness = await vector_store.check_vector_readiness()
        print(f"   ‚úÖ Vector readiness check: {readiness}")
        
        # Test alternative insights when not ready
        if not vector_store.vector_operations_enabled:
            insights = await vector_store.provide_general_insights("test query")
            print(f"   ‚úÖ Alternative insights provided: {len(insights.get('alternative_insights', {}))} features")
        
        # Test similarity search with graceful degradation
        similarity_result = await vector_store.similarity_search("test resume text")
        print(f"   ‚úÖ Similarity search: enabled={similarity_result.get('enabled', False)}")
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå Vector store test failed: {str(e)}")
        return False

async def test_iterative_analysis_manager():
    """Test iterative analysis manager"""
    print("üß™ Testing Iterative Analysis Manager...")
    
    try:
        from services.iterative_analysis_manager import IterativeAnalysisManager
        
        # Mock dependencies
        class MockAssessmentCoordinator:
            async def process(self, input_data):
                class MockResponse:
                    def __init__(self):
                        self.success = True
                        self.data = {
                            "executive_summary": {
                                "overall_score": 85,
                                "recommendation": "conditional_hire",
                                "key_strengths": ["Strong technical skills", "Good experience"]
                            }
                        }
                        self.confidence = 0.88
                        self.processing_time = 15.2
                return MockResponse()
        
        class MockMarketService:
            pass
        
        # Initialize manager
        manager = IterativeAnalysisManager(
            MockAssessmentCoordinator(),
            MockMarketService()
        )
        
        # Test preferences customization
        preferences = {
            "roadmapDuration": 12,
            "careerGoals": "job_switch",
            "learningTimeCommitment": 10,
            "priorityAreas": ["Technical Skills", "Resume Quality"]
        }
        
        config = manager.customize_analysis_config(preferences)
        print(f"   ‚úÖ Analysis config customized: {len(config)} parameters")
        
        # Test personalized insights generation
        mock_analysis = {
            "final_assessment": {
                "executive_summary": {
                    "key_strengths": ["Python", "Machine Learning"],
                    "overall_score": 85
                }
            }
        }
        
        insights = manager.generate_personalized_insights(mock_analysis, preferences)
        print(f"   ‚úÖ Personalized insights: {len(insights)} categories")
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå Iterative analysis manager test failed: {str(e)}")
        return False

async def test_assessment_coordinator_enhancements():
    """Test assessment coordinator enhancements"""
    print("üß™ Testing Assessment Coordinator Enhancements...")
    
    try:
        from ai_agents.assessment_coordinator import AssessmentCoordinator
        from ai_agents.vector_store import VectorStoreManager
        
        # Mock Groq client
        class MockGroqClient:
            def __init__(self):
                self.chat = self
                self.completions = self
            
            async def create(self, **kwargs):
                class MockResponse:
                    def __init__(self):
                        self.choices = [
                            type('Choice', (), {
                                'message': type('Message', (), {
                                    'content': '{"executive_summary": {"overall_score": 85, "recommendation": "conditional_hire"}}'
                                })()
                            })()
                        ]
                return MockResponse()
        
        # Initialize coordinator
        vector_store = VectorStoreManager()
        coordinator = AssessmentCoordinator(
            groq_client=MockGroqClient(),
            vector_store=vector_store
        )
        
        # Test vector readiness check
        readiness = await coordinator._check_vector_readiness()
        print(f"   ‚úÖ Vector readiness check: enabled={readiness.get('enabled', False)}")
        
        # Test enhanced input processing
        input_data = {
            "resume_text": "Sample resume text with Python and machine learning experience",
            "job_description": "Software engineer position requiring Python skills",
            "user_id": "test_user_123",
            "analysis_context": {
                "user_preferences": {
                    "careerGoals": "job_switch",
                    "roadmapDuration": 12,
                    "priorityAreas": ["Technical Skills"]
                },
                "iteration_type": "initial"
            }
        }
        
        print(f"   ‚úÖ Enhanced input data prepared with {len(input_data)} fields")
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå Assessment coordinator test failed: {str(e)}")
        return False

def test_frontend_components():
    """Test frontend component imports"""
    print("üß™ Testing Frontend Components...")
    
    try:
        # Get the parent directory (project root)
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
        # Check if component files exist
        components = [
            "components/PreAnalysisForm.tsx",
            "components/IterativeFeedbackForm.tsx", 
            "components/VectorReadinessNotification.tsx"
        ]
        
        for component in components:
            component_path = os.path.join(project_root, component)
            if os.path.exists(component_path):
                print(f"   ‚úÖ {component} exists")
            else:
                print(f"   ‚ùå {component} missing")
                return False
        
        # Check AI utils
        ai_utils_path = os.path.join(project_root, "lib/ai-utils.ts")
        if os.path.exists(ai_utils_path):
            print("   ‚úÖ AI utils library exists")
        else:
            print("   ‚ùå AI utils library missing")
            return False
        
        # Check test page
        test_page_path = os.path.join(project_root, "app/test-enhanced-analysis/page.tsx")
        if os.path.exists(test_page_path):
            print("   ‚úÖ Test page created")
        else:
            print("   ‚ùå Test page missing")
            return False
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå Frontend components test failed: {str(e)}")
        return False

def test_api_models():
    """Test API models"""
    print("üß™ Testing API Models...")
    
    try:
        from models import PreAnalysisInput, AnalysisPreferences, FeedbackRequest
        
        # Test PreAnalysisInput
        pre_analysis = PreAnalysisInput(
            resume_text="Sample resume",
            preferences={
                "roadmapDuration": 12,
                "careerGoals": "job_switch"
            }
        )
        print("   ‚úÖ PreAnalysisInput model works")
        
        # Test AnalysisPreferences
        preferences = AnalysisPreferences(
            roadmapDuration=12,
            careerGoals="job_switch",
            priorityAreas=["Technical Skills"]
        )
        print("   ‚úÖ AnalysisPreferences model works")
        
        # Test FeedbackRequest
        feedback = FeedbackRequest(
            feedback_type="refinement",
            feedback_text="Need more focus on Python skills",
            feedback_data={"satisfaction": 7},
            improvement_areas=["Skill Recommendations"]
        )
        print("   ‚úÖ FeedbackRequest model works")
        
        return True
        
    except Exception as e:
        print(f"   ‚ùå API models test failed: {str(e)}")
        return False

async def run_comprehensive_test():
    """Run all tests"""
    print("üöÄ Starting Enhanced Pipeline Comprehensive Test")
    print("=" * 60)
    
    test_results = []
    
    # Test 1: Vector Store Critical Mass
    result1 = await test_vector_store_critical_mass()
    test_results.append(("Vector Store Critical Mass", result1))
    
    # Test 2: Iterative Analysis Manager
    result2 = await test_iterative_analysis_manager()
    test_results.append(("Iterative Analysis Manager", result2))
    
    # Test 3: Assessment Coordinator Enhancements
    result3 = await test_assessment_coordinator_enhancements()
    test_results.append(("Assessment Coordinator Enhancements", result3))
    
    # Test 4: Frontend Components
    result4 = test_frontend_components()
    test_results.append(("Frontend Components", result4))
    
    # Test 5: API Models
    result5 = test_api_models()
    test_results.append(("API Models", result5))
    
    # Summary
    print("\n" + "=" * 60)
    print("üìä TEST RESULTS SUMMARY")
    print("=" * 60)
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status} {test_name}")
        if result:
            passed += 1
    
    print(f"\nüéØ Overall Result: {passed}/{total} tests passed ({(passed/total)*100:.1f}%)")
    
    if passed == total:
        print("üéâ ALL TESTS PASSED! Enhanced pipeline is ready for production.")
        print("\nüìã What's Working:")
        print("   ‚úÖ Vector database with critical mass strategy (1000 resume threshold)")
        print("   ‚úÖ Graceful degradation when competitive analysis isn't ready")
        print("   ‚úÖ User preferences collection and processing")
        print("   ‚úÖ Iterative analysis with feedback refinement")
        print("   ‚úÖ Enhanced assessment coordinator with context awareness")
        print("   ‚úÖ Frontend components for enhanced UX")
        print("   ‚úÖ API models for new endpoints")
        
        print("\nüöÄ Ready to Test:")
        print("   1. Start backend: python3 run.py")
        print("   2. Start frontend: npm run dev")
        print("   3. Visit: http://localhost:3000/test-enhanced-analysis")
        
    else:
        print("‚ö†Ô∏è  Some tests failed. Please review the errors above.")
    
    return passed == total

if __name__ == "__main__":
    # Run the comprehensive test
    success = asyncio.run(run_comprehensive_test())
    sys.exit(0 if success else 1) 